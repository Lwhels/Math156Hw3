{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Lwhels/Math156Hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a program to train a binary logistic regression model using mini-batch SGD. Use the logistic\n",
    "# regression model we derived in class, corresponding to Equation (4.90) from the textbook, and where\n",
    "# the feature transformation ϕ is the identity function.\n",
    "\n",
    "# The program should include the following hyperparameters:\n",
    "# • Batch size\n",
    "# • Fixed learning rate\n",
    "# • Maximum number of iterations\n",
    "\n",
    "# The program should output the following:\n",
    "# • The learned weights w\n",
    "# • The learned bias b\n",
    "# • The final training loss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "EPSILON = 1e-9\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    z: input data\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Predict labels\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    X: input data\n",
    "    w: weights\n",
    "    b: bias\n",
    "    \"\"\"\n",
    "    z = np.dot(X, w) + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "# Compute loss\n",
    "def loss(y, a):\n",
    "    \"\"\"\n",
    "    y: true labels\n",
    "    a: predicted labels\n",
    "    \"\"\"\n",
    "    return -np.mean(y * np.log(a + EPSILON) + (1 - y) * np.log(1 - a + EPSILON))\n",
    "\n",
    "# Compute gradients\n",
    "def compute_gradients(X, a, y, batch_size):\n",
    "    \"\"\"\n",
    "    X: input data\n",
    "    a: predicted labels\n",
    "    y: true labels\n",
    "    batch_size: batch size\n",
    "    \"\"\"\n",
    "    dw = np.dot(X.T, (a - y)) / batch_size\n",
    "    db = np.sum(a - y) / batch_size\n",
    "    return dw, db\n",
    "\n",
    "# Train with mini-batch SGD\n",
    "def train_with_sgd(X, y, w, b, batch_size, max_iters, learning_rate):\n",
    "    \"\"\"\n",
    "    X: input data\n",
    "    y: true labels\n",
    "    w: weights\n",
    "    b: bias\n",
    "    batch_size: batch size\n",
    "    max_iters: maximum number of iterations\n",
    "    learning_rate: fixed learning rate\n",
    "    \"\"\"\n",
    "    for i in range(max_iters):\n",
    "        # Randomly sample a batch of data\n",
    "        indices = np.random.choice(X.shape[0], batch_size, replace=False)\n",
    "        X_batch = X[indices]\n",
    "        y_batch = y[indices]\n",
    "        # Compute gradients based on the batch predictions\n",
    "        a = predict(X_batch, w, b)\n",
    "        dw, db = compute_gradients(X_batch, a, y_batch, batch_size)\n",
    "        # Update weights and bias\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        if i % 10 == 0:  # Print loss every 100 iterations\n",
    "            print(f\"Iteration {i}, Loss: {loss(y_batch, a):.4f}\")\n",
    "    \n",
    "    final_loss = loss(y, predict(X, w, b))\n",
    "    return w, b, final_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Class 0: 77\n",
      "Class 1: 122\n",
      "Validation Set\n",
      "Class 0: 72\n",
      "Class 1: 127\n",
      "Iteration 0, Loss: 2.7426\n",
      "Iteration 10, Loss: 1.2224\n",
      "Iteration 20, Loss: 1.6434\n",
      "Iteration 30, Loss: 2.5950\n",
      "Iteration 40, Loss: 1.6533\n",
      "Iteration 50, Loss: 1.6693\n",
      "Iteration 60, Loss: 0.7248\n",
      "Iteration 70, Loss: 1.5207\n",
      "Iteration 80, Loss: 1.0582\n",
      "Iteration 90, Loss: 0.4741\n",
      "Iteration 100, Loss: 1.0768\n",
      "Iteration 110, Loss: 1.2715\n",
      "Iteration 120, Loss: 0.7635\n",
      "Iteration 130, Loss: 0.2968\n",
      "Iteration 140, Loss: 0.4998\n",
      "Iteration 150, Loss: 0.3795\n",
      "Iteration 160, Loss: 1.0238\n",
      "Iteration 170, Loss: 0.3794\n",
      "Iteration 180, Loss: 0.2548\n",
      "Iteration 190, Loss: 0.3576\n",
      "Iteration 200, Loss: 0.5790\n",
      "Iteration 210, Loss: 0.2386\n",
      "Iteration 220, Loss: 0.4439\n",
      "Iteration 230, Loss: 0.3025\n",
      "Iteration 240, Loss: 0.4022\n",
      "Iteration 250, Loss: 0.2888\n",
      "Iteration 260, Loss: 0.1602\n",
      "Iteration 270, Loss: 0.7007\n",
      "Iteration 280, Loss: 0.7724\n",
      "Iteration 290, Loss: 0.5859\n",
      "Iteration 300, Loss: 0.2282\n",
      "Iteration 310, Loss: 0.3270\n",
      "Iteration 320, Loss: 0.1556\n",
      "Iteration 330, Loss: 0.1782\n",
      "Iteration 340, Loss: 0.1992\n",
      "Iteration 350, Loss: 0.1290\n",
      "Iteration 360, Loss: 0.4175\n",
      "Iteration 370, Loss: 0.3513\n",
      "Iteration 380, Loss: 0.3043\n",
      "Iteration 390, Loss: 0.4789\n",
      "Iteration 400, Loss: 0.1586\n",
      "Iteration 410, Loss: 0.1189\n",
      "Iteration 420, Loss: 0.2298\n",
      "Iteration 430, Loss: 0.3896\n",
      "Iteration 440, Loss: 0.1835\n",
      "Iteration 450, Loss: 0.1997\n",
      "Iteration 460, Loss: 0.1151\n",
      "Iteration 470, Loss: 0.1359\n",
      "Iteration 480, Loss: 0.3971\n",
      "Iteration 490, Loss: 0.1212\n",
      "Iteration 500, Loss: 0.2812\n",
      "Iteration 510, Loss: 0.2497\n",
      "Iteration 520, Loss: 0.1093\n",
      "Iteration 530, Loss: 0.0777\n",
      "Iteration 540, Loss: 0.2632\n",
      "Iteration 550, Loss: 0.0739\n",
      "Iteration 560, Loss: 0.1132\n",
      "Iteration 570, Loss: 0.3001\n",
      "Iteration 580, Loss: 0.2457\n",
      "Iteration 590, Loss: 0.2572\n",
      "Iteration 600, Loss: 0.0577\n",
      "Iteration 610, Loss: 0.1134\n",
      "Iteration 620, Loss: 0.0748\n",
      "Iteration 630, Loss: 0.0620\n",
      "Iteration 640, Loss: 0.0862\n",
      "Iteration 650, Loss: 0.2579\n",
      "Iteration 660, Loss: 0.2191\n",
      "Iteration 670, Loss: 0.1006\n",
      "Iteration 680, Loss: 0.2086\n",
      "Iteration 690, Loss: 0.1831\n",
      "Iteration 700, Loss: 0.0573\n",
      "Iteration 710, Loss: 0.1033\n",
      "Iteration 720, Loss: 0.0935\n",
      "Iteration 730, Loss: 0.2642\n",
      "Iteration 740, Loss: 0.0969\n",
      "Iteration 750, Loss: 0.0848\n",
      "Iteration 760, Loss: 0.0893\n",
      "Iteration 770, Loss: 0.1415\n",
      "Iteration 780, Loss: 0.0928\n",
      "Iteration 790, Loss: 0.0561\n",
      "Iteration 800, Loss: 0.0395\n",
      "Iteration 810, Loss: 0.0834\n",
      "Iteration 820, Loss: 0.0326\n",
      "Iteration 830, Loss: 0.2575\n",
      "Iteration 840, Loss: 0.1905\n",
      "Iteration 850, Loss: 0.0437\n",
      "Iteration 860, Loss: 0.2660\n",
      "Iteration 870, Loss: 0.0507\n",
      "Iteration 880, Loss: 0.0618\n",
      "Iteration 890, Loss: 0.0995\n",
      "Iteration 900, Loss: 0.1658\n",
      "Iteration 910, Loss: 0.1105\n",
      "Iteration 920, Loss: 0.0614\n",
      "Iteration 930, Loss: 0.1020\n",
      "Iteration 940, Loss: 0.2362\n",
      "Iteration 950, Loss: 0.2256\n",
      "Iteration 960, Loss: 0.0876\n",
      "Iteration 970, Loss: 0.0752\n",
      "Iteration 980, Loss: 0.0913\n",
      "Iteration 990, Loss: 0.0583\n",
      "\n",
      "Final Training Loss: 0.10011016476489475\n",
      "\n",
      "Model Performance on Test Set:\n",
      "Accuracy:  0.9766\n",
      "Precision: 0.9906\n",
      "Recall:    0.9722\n",
      "F1-score:  0.9813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# n this problem, you will run a logistic regression model for classification on a breast cancer dataset\n",
    "# (a) Download the Wisconsin Breast Cancer dataset from the UCI Machine Learning Repository 1 or\n",
    "# scikit-learn’s built-in datasets 2.\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# (b) Split the dataset into train, validation, and test sets.\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# (c) Report the size of each class in your training (+ validation) set.\n",
    "# Report the size of each class in the training (+ validation) set\n",
    "print('Training Set')\n",
    "print('Class 0:', np.sum(y_train == 0))\n",
    "print('Class 1:', np.sum(y_train == 1))\n",
    "print('Validation Set')\n",
    "print('Class 0:', np.sum(y_val == 0))\n",
    "print('Class 1:', np.sum(y_val == 1))\n",
    "\n",
    "# (d) Train a binary logistic regression model using your implementation from problem 3. Initialize\n",
    "# the model weights randomly, sampling from a standard Gaussian distribution. Experiment with\n",
    "# different choices of fixed learning rate and batch size.\n",
    "\n",
    "# Initialize the weights randomly\n",
    "np.random.seed(42)\n",
    "w = np.random.randn(X_train.shape[1])\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "max_iters = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Train the model using mini-batch SGD\n",
    "w, b, final_loss = train_with_sgd(X_train, y_train, w, b, batch_size, max_iters, learning_rate)\n",
    "print(\"\\nFinal Training Loss:\", final_loss)\n",
    "\n",
    "\n",
    "# (e) Use the trained model to report the performance of the model on the test set. For evaluation\n",
    "# metrics, use accuracy, precision, recall, and F1-score.\n",
    "\n",
    "# Use the learned weights to predict on the test set\n",
    "z = predict(X_test, w, b)\n",
    "y_pred = np.round(z)\n",
    "\n",
    "# Evaluate the model using accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Display results on the test set\n",
    "print(\"\\nModel Performance on Test Set:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "\n",
    "# (f) Summarize your findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
